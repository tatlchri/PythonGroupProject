{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "BS1819-1617  \n",
    "Data Structures and Algorithms  \n",
    "Group Assignment  \n",
    "</b>\n",
    "\n",
    "Team 3:  \n",
    "  * Ahmad Bilal Aslam  \n",
    "  * Chris Ying  \n",
    "  * Christina Lefkothea Tatli   \n",
    "  * Joh B  \n",
    "  * Kelvin Goh  \n",
    "  * Selly Salkha  \n",
    "\n",
    "\n",
    "<b><u> Part 4.1 (Extra part 1) </u></b>\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## PART ONE, PART TWO and EXTRA PART ONE code - to remove this when merging all the Jupyter notebooks together ##\n",
    "#################################################################################################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "##### START OF PART 1 - calculate daily returns #####\n",
    "\n",
    "##~~ KEY VARIABLE: \"returns\" stores daily returns of stocks\n",
    "\n",
    "##### To Read company names into a dictionary\n",
    "def readNamesIntoDict():\n",
    "    d = dict()\n",
    "    input_file = csv.DictReader(open(\"SP_500_firms.csv\"))\n",
    "    for row in input_file:\n",
    "        #print(row)\n",
    "        d[row['Symbol']] = [row['Name'],row['Sector']]\n",
    "    return d\n",
    "\n",
    "##### To calculate daily returns from stock prices\n",
    "def returns_Stocks(priceData):      \n",
    "    # input is a pd.dataframe of stock prices\n",
    "    # output is a pd.dataframe of daily returns    \n",
    "    returns = priceData.pct_change()\n",
    "    # remove index 0 of returns - it's a nan value because the first period data has no daily return\n",
    "    #returns = returns[1:len(returns)]\n",
    "    # We had used the built-in function, the next function below is the manual calculation\n",
    "    return returns\n",
    "\n",
    "def returns_Stocks_manual_calc(priceData):      \n",
    "    # Manual calculation of the returns\n",
    "    returns = priceData / priceData.shift(1) - 1\n",
    "    # remove index 0 of returns - it's a nan value because the first period data has no daily return\n",
    "    #returns = returns[1:len(returns)]\n",
    "    return returns\n",
    "\n",
    "# make sure that the manual calculation is same as built-in function\n",
    "def test_returns_Stock_built_in_equals_manual(priceData):  \n",
    "    returns_built_in = returns_Stocks(priceData)\n",
    "    returns_manual = returns_Stocks_manual_calc(priceData)\n",
    "    difference = returns_built_in - returns_manual \n",
    "    print(\"The total difference between built-in and manual way of calculating daily returns, across stocks and time period, is:\", difference.sum().sum()) \n",
    "\n",
    "##### Several functions to determine which stock has max, min return; which are overall best, worst stocks; and max and min std of daily returns\n",
    "def max_return(returns, namesDict = readNamesIntoDict()):    \n",
    "    # input: pd.dataframe of daily returns, and a dictionary of company/sector (call the readNamesIntoDict() function if the dictionary is not passed as an argument)\n",
    "    # output: returns the maximum daily return along with company name and Sector\n",
    "    maxDaily_byComp  = returns.max()            #Maximum returns for each company\n",
    "\n",
    "    maxDaily = maxDaily_byComp.max()            #Overall highest daily return\n",
    "    maxDaily_CompSym = maxDaily_byComp.idxmax() #Getting index of the maximum return value\n",
    "\n",
    "    maxDaily_CompName = namesDict[maxDaily_CompSym][0]  #Company Name using its symbol\n",
    "    maxDaily_Sector = namesDict[maxDaily_CompSym][1]    #Company Sector\n",
    "    \n",
    "    return maxDaily_Sector, maxDaily_CompName, maxDaily\n",
    "    \n",
    "\n",
    "def min_return (returns, namesDict = readNamesIntoDict()):    \n",
    "    # input: pd.dataframe of daily returns, and a dictionary of company/sector (call the readNamesIntoDict() function if the dictionary is not passed as an argument)\n",
    "    # output: returns the minimum daily return along with company name and Sector\n",
    "\n",
    "    minDaily_byComp  = returns.min()            #Minimum returns for each company\n",
    "\n",
    "    minDaily = minDaily_byComp.min()            #Overall lowest daily return\n",
    "    minDaily_CompSym = minDaily_byComp.idxmin() #Getting index of the minimum return value\n",
    "\n",
    "    minDaily_CompName = namesDict[minDaily_CompSym][0]  #Company Name using its symbol\n",
    "    minDaily_Sector = namesDict[minDaily_CompSym][1]    #Company Sector\n",
    "    \n",
    "    return minDaily_Sector, minDaily_CompName, minDaily\n",
    "\n",
    "\n",
    "def overall_best (priceData, namesDict = readNamesIntoDict()):    \n",
    "    # input: pd.dataframe of price data, and a dictionary of company/sector (call the readNamesIntoDict() function if the dictionary is not passed as an argument)\n",
    "    # output: returns the maximum yearly return along with company name and Sector\n",
    "    overallReturn_byComp  = priceData.iloc[-1] / priceData.iloc[0] - 1   #yearly returns for each company\n",
    "\n",
    "    overallBest = overallReturn_byComp.max()            #best yearly return\n",
    "    overallBest_CompSym = overallReturn_byComp.idxmax() #Getting index of the best yearly return\n",
    "\n",
    "     #Loading Company Symbols mapping into namesDict\n",
    "    overallBest_CompName = namesDict[overallBest_CompSym][0]  #Company Name using its symbol\n",
    "    overallBest_Sector = namesDict[overallBest_CompSym][1]    #Company Sector\n",
    "    \n",
    "    return overallBest_Sector, overallBest_CompName, overallBest\n",
    "    \n",
    "\n",
    "def overall_worst (priceData, namesDict = readNamesIntoDict()):    \n",
    "    # input: pd.dataframe of price data, and a dictionary of company/sector (call the readNamesIntoDict() function if the dictionary is not passed as an argument)\n",
    "    # output: returns the minimum yearly return along with company name and Sector\n",
    "    overallReturn_byComp  = priceData.iloc[-1] / priceData.iloc[0] - 1   #yearly returns for each company\n",
    "\n",
    "    overallWorst = overallReturn_byComp.min()            #worst yearly return\n",
    "    overallWorst_CompSym = overallReturn_byComp.idxmin() #Getting index of the worst yearly return\n",
    "\n",
    "    overallWorst_CompName = namesDict[overallWorst_CompSym][0]  #Company Name using its symbol\n",
    "    overallWorst_Sector = namesDict[overallWorst_CompSym][1]    #Company Sector\n",
    "    \n",
    "    return overallWorst_Sector, overallWorst_CompName, overallWorst\n",
    "\n",
    "\n",
    "def max_std (returns, namesDict = readNamesIntoDict()):    \n",
    "    # input: pd.dataframe of price data, and a dictionary of company/sector (call the readNamesIntoDict() function if the dictionary is not passed as an argument)\n",
    "    # output: returns the maximum std. dev along with company name and Sector\n",
    "    std_byComp  = returns.std()            #std. dev of returns for each company\n",
    "\n",
    "    maxStd = std_byComp.max()            #maximum std. dev\n",
    "    maxStd_CompSym = std_byComp.idxmax() #Getting index of the maximum std. dev\n",
    "\n",
    "    namesDict = readNamesIntoDict() #Loading Company Symbols mapping into namesDict\n",
    "    maxStd_CompName = namesDict[maxStd_CompSym][0]  #Company Name using its symbol\n",
    "    maxStd_Sector = namesDict[maxStd_CompSym][1]    #Company Sector\n",
    "    \n",
    "    return maxStd_Sector, maxStd_CompName, maxStd\n",
    "    \n",
    "    \n",
    "def min_std (returns, namesDict = readNamesIntoDict()):    \n",
    "    # input: pd.dataframe of price data, and a dictionary of company/sector (call the readNamesIntoDict() function if the dictionary is not passed as an argument)\n",
    "    # output: returns the minimum std. dev along with company name and Sector\n",
    "    std_byComp  = returns.std()            #std. dev of returns for each company\n",
    "\n",
    "    minStd = std_byComp.min()            #minimum std. dev\n",
    "    minStd_CompSym = std_byComp.idxmin() #Getting index of the minimum std. dev\n",
    "\n",
    "    minStd_CompName = namesDict[minStd_CompSym][0]  #Company Name using its symbol\n",
    "    minStd_Sector = namesDict[minStd_CompSym][1]    #Company Sector\n",
    "    \n",
    "    return minStd_Sector, minStd_CompName, minStd\n",
    "\n",
    "\n",
    "##### Read company names into a dictionary\n",
    "namesDict = readNamesIntoDict()\n",
    "\n",
    "##### Read Prices Data into pandas\n",
    "filename = 'SP_500_close_2015.csv'\n",
    "priceData = pd.read_csv(filename,index_col = 0)\n",
    "\n",
    "##### Call the function to calculate stocks' daily returns from the price data\n",
    "returns = returns_Stocks (priceData)\n",
    "\n",
    "# test that manual and built-in calculations are the same\n",
    "# uncomment the next line of code to run test\n",
    "#test_returns_Stock_built_in_equals_manual(priceData)\n",
    "\n",
    "\n",
    "##### END OF PART 1 #####\n",
    "\n",
    "\n",
    "\n",
    "##### START OF PART 2 - find correlations between stocks' daily returns #####\n",
    "\n",
    "##~~ KEY VARIABLE: \"correlationTable\" stores correlation results \n",
    "\n",
    "##### Generate pairwise correlation table using panda\n",
    "def corTable(returns):\n",
    "    # this uses the built-in function. The manual calculation is as below.\n",
    "    return returns.corr()\n",
    "    \n",
    "# store correlation results (to use as input to other functions)\n",
    "correlationTable = corTable(returns) \n",
    "\n",
    "##### read company names into pd dataframe\n",
    "compData = pd.read_csv('SP_500_firms.csv', index_col = 0)\n",
    "\n",
    "##### Print correlation between company A and B\n",
    "def printCor(correlationTable, companyA, companyB):\n",
    "    corr = correlationTable.loc[companyA,companyB]\n",
    "    nameA = compData.loc[companyA,'Name']\n",
    "    nameB = compData.loc[companyB,'Name']\n",
    "    return nameA, nameB, corr\n",
    "\n",
    "##### Compare panda method and python manual method of calculating correlations\n",
    "def testCor_pairwise(correlationTable, companyA, companyB):\n",
    "    print('Panda method:')\n",
    "    print(correlationTable.loc[companyA,companyB])\n",
    "    print('Standard data structure method')\n",
    "    returns_for_manual = returns[1:len(returns)]\n",
    "    a,b = np.array(returns_for_manual.get(companyA).tolist(),dtype = float),np.array(returns_for_manual.get(companyB).tolist(),dtype = float)\n",
    "    print(np.sum((a - np.mean(a))/np.std(a)*(b - np.mean(b))/np.std(b))/(len(a)))\n",
    "\n",
    "##### The above does a pairwise correlation manually\n",
    "#     This chunk of code computes all pairwise correlations manually\n",
    "def testCor_allprices(returns):\n",
    "    # copy returns dataframe to initialise corr_matrix. Will edit the cell contents in code below.\n",
    "    corr_matrix = returns.copy()\n",
    "    col_names = list(returns.columns.values)\n",
    "    # remove index 0 of returns - it's a nan value because the first period data has no daily return\n",
    "    returns = returns[1:len(returns)]\n",
    "    for i in range(len(col_names)):\n",
    "        for j in range(i, len(col_names)):\n",
    "            companyA = col_names[i]\n",
    "            companyB = col_names[j]\n",
    "            a,b = np.array(returns.get(companyA).tolist(),dtype = float),np.array(returns.get(companyB).tolist(),dtype = float)\n",
    "            corr_matrix.ix[companyA, companyB] = (np.sum((a - np.mean(a))/np.std(a)*(b - np.mean(b))/np.std(b))/(len(a)))\n",
    "            corr_matrix.ix[companyB, companyA] = (np.sum((a - np.mean(a))/np.std(a)*(b - np.mean(b))/np.std(b))/(len(a)))\n",
    "    return corr_matrix\n",
    "\n",
    "\n",
    "##### make sure that the manual calculation is same as built-in function\n",
    "def test_correlation_Stock_built_in_equals_manual(returns):  \n",
    "    corr_built_in = corTable(returns)\n",
    "    corr_manual = testCor_allprices(returns)\n",
    "    difference = corr_built_in - corr_manual \n",
    "    print(\"The total difference between built-in and manual way of calculating correlations across stocks is:\", difference.sum().sum()) \n",
    "\n",
    "# To test if built-in and manual way of finding correlations are the same\n",
    "# Uncomment the lines below to test it. (have tested, values returned are the same. There is a very small difference 3.1778630833582955e-12, for the manual and built-in way of computing all correlations, likely due to rounding errors)\n",
    "#testCor_pairwise(correlationTable, 'GOOGL', 'FB')\n",
    "#test_correlation_Stock_built_in_equals_manual(returns)\n",
    "\n",
    "    \n",
    "##### List top and bottom correlated companies of a company   \n",
    "def top_bottom_Cor(correlationTable,company):\n",
    "    print('Finding the top and bottom correlated companies for ', company, ':')\n",
    "    print('===================================================')\n",
    "    min = correlationTable[company].sort_values()[0:5]\n",
    "    max = correlationTable[company].sort_values(ascending=False)[1:6]\n",
    "    list1 = []\n",
    "    list2 = []\n",
    "    for i in min.index:\n",
    "        list1.append(compData.loc[i,'Name'])\n",
    "    for i in max.index:\n",
    "        list2.append(compData.loc[i,'Name'])\n",
    "    min.index = list1\n",
    "    max.index = list2\n",
    "    print('Bottom correlated :')\n",
    "    print('-----------------')\n",
    "    print(min)\n",
    "    print('') # break line\n",
    "    print('Top correlated:')\n",
    "    print('---------------')\n",
    "    print(max)\n",
    "\n",
    "\n",
    "##### END OF PART 2 #####\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "<b>In-depth analysis</b>\n",
    "\n",
    "The project is \"open\" in the sense that you can probably think of further interesting questions to look into based on returns, correlations, and clusters. This is not required but being creative and going further than the above questions will make your work stand out. You can explore one or several of the ideas below, or come up with questions of your own. Depending on your interests, you might look at di\u001bfferent things. For example, when researching the algorithm, you might be interested in its complexity, and how to improve your implementation's e\u001e",
    "fficiency. There is scope to go quite a bit deeper here. \n",
    "\n",
    "* Please refer to the more efficient code in the cell below.\n",
    "\n",
    "Brief description of the algorithm:  \n",
    "\n",
    "* Uses a list of n nodes, instead of a dictionary.  \n",
    "  \n",
    "  \n",
    "* Each node has these properties:  \n",
    "    + parent - stores the node's parent, similar to the described Part 3 algorithm. Nodes in the same tree belong to the same cluster  \n",
    "    + rank - stores the depth of the node. i.e. root of the tree has rank of 0.  \n",
    "    + setlist - maintains a list of nodes that belong in the same cluster. the list is updated whenever trees/clusters are merged. \n",
    "\n",
    "\n",
    "* MakeSet function\n",
    "    + initialises each node's parent to itself (similar to the described algorithm for Part 3) \n",
    "    + initialise each node's rank to 0 (each node is a root [tree with only 1 node])\n",
    "    \n",
    "    \n",
    "* Union function  \n",
    "    + When merging node x and node y, find the root for node x and the root for node y (similar to the described algorithm for part 3)\n",
    "    + While the described algorithm in Part 3 says \"We then make one point to the other\", here's what we do differently here. Instead of arbitrarily select one node to point to another node, we check the rank of x and y. We always make the smaller tree merge with the larger tree. In this way, we ensure that the depth of the tree will be O(log n), i.e. a balanced tree. If we arbitrarily select which node to point to another, we risk having a very long chain / linked list, that can be O(n).\n",
    "    \n",
    "    \n",
    "* Find function\n",
    "    + This function is called from the Union function, when finding the root of node x and node y.\n",
    "    + This function basically starts from the starting node, and follow the node's parent, and that parent etc until we reach the root of the tree\n",
    "    + What is different, is that as it transverses the chain, it sets the current node's parent, to the root of its parent (i.e. the root of the tree). This is path compression which makes subsequent Find (and hence Union), quicker.  \n",
    "    \n",
    "Discussion of efficiency:\n",
    "\n",
    "* This should have better space efficiency as it uses a list rather than a dictionary. A dictionary / hash table, would typically have redundant space allocation in order to minimise collisions. Nonetheless, both would be O(n) space.\n",
    "\n",
    "\n",
    "* In the described algorithm, the time efficiency would be k \\* n, because we perform k operations, and in each operation, the transversing of the chain could be O(n) time.  In our algorithm, each transversal of the chain is O(log n), because we ensure that the tree is balanced. So it is O(k log n). Since k is at max the total number of edges, which is O(n^2) since the graph is dense, our algorithm is O(n^2 * log n) instead of O(n ^ 3).\n",
    "\n",
    "\n",
    "* But in fact, by using path compression and balanced trees, the running time is actually O(k log* n). \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### START OF PART 4 part 1 - more efficient clustering algorithm #####\n",
    "\n",
    "##~~ KEY VARIABLE: nodeList stores a list of Nodes, each node representing a stock price. Find(nodeList[i]).getSet()) will return the set of nodes that belong to the same cluster as nodeList[i]\n",
    "\n",
    "##### Store all stock names into a list\n",
    "all_stock_names = list(correlationTable.columns.values)\n",
    "n = len(all_stock_names)\n",
    "\n",
    "# COMMENT: The length of namesDict and all_stock_names \n",
    "#          DO NOT MATCH! all_stock_names is correct, \n",
    "#          there are 496 stock names in the price csv file\n",
    "#          len(namesDict) shows 504, which matches \n",
    "#          the firms csv file\n",
    "#          So the results differ because of the csv files.\n",
    "#          To check that the usage of namesDict is correct.\n",
    "\n",
    "##### Add all pairwise correlation (upper-triangle matrix), into a list\n",
    "edge_list = []\n",
    "for i in range(len(all_stock_names)-1):\n",
    "    for j in range(i+1, len(all_stock_names)):\n",
    "        # add tuple (weight, source, destination) to list\n",
    "        edge_list.append( (correlationTable.loc[all_stock_names[i], all_stock_names[j]], i, j) )\n",
    "\n",
    "# sort them in descending order\n",
    "edge_list = sorted(edge_list, reverse = True)\n",
    "\n",
    "##### Create a node class with:\n",
    "#       node_name: prints the stock's name\n",
    "#       parent: nodes with the same parent will belong to the same cluster\n",
    "#       rank: depth of the node. Use this for efficient merging - tries to keep tree as balanced as possible - shorter search time\n",
    "#       setlist: stores the names of all the nodes that belong to the same cluster. Updates this when sets are merged \n",
    "#                (the root of the tree at any time will always store the complete set of nodes belonging to the same cluster)\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, node_name):\n",
    "        self.name = node_name\n",
    "        # maintain a list that stores all the nodes\n",
    "        # in the same cluster. Update this list when\n",
    "        # clusters merge (i.e. update the root node list)\n",
    "        self.setlist = [node_name];\n",
    "    \n",
    "    def setParent(self, node):\n",
    "        self.parent = node\n",
    "        \n",
    "    def getParent(self):\n",
    "        return self.parent\n",
    "        \n",
    "    def setRank(self, rank):\n",
    "        self.rank = rank\n",
    "\n",
    "    def getRank(self):\n",
    "        return self.rank\n",
    "        \n",
    "    def getSet(self):\n",
    "        return self.setlist\n",
    "        \n",
    "    def mergeSet(self, node):\n",
    "        # update the root node list whenever there is a merge operation. Root of the tree will have the complete set list\n",
    "        node.getSet().extend(self.setlist)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    \n",
    "\n",
    "def MakeSet(x):\n",
    "    x.setParent(x)\n",
    "    x.setRank(0)\n",
    "    \n",
    "def Union(x, y):\n",
    "    # Union in a way such that the depth of the tree is minimised\n",
    "    # always add the shorter tree under the deeper tree so that the height doesn't increase\n",
    "    xRoot = Find(x)\n",
    "    yRoot = Find(y)\n",
    "    if xRoot == yRoot:\n",
    "        return\n",
    "\n",
    "    # x and y are not already in same set. Merge them.\n",
    "    # always merge the smaller subtree into the bigger one\n",
    "    # to minimise the tree height\n",
    "    if xRoot.getRank() < yRoot.getRank():\n",
    "        xRoot.setParent(yRoot)\n",
    "        xRoot.mergeSet(yRoot)\n",
    "    elif xRoot.getRank() > yRoot.getRank():\n",
    "        yRoot.setParent(xRoot)\n",
    "        yRoot.mergeSet(xRoot)\n",
    "    else:\n",
    "        yRoot.setParent(xRoot)\n",
    "        yRoot.mergeSet(xRoot)\n",
    "        xRoot.setRank(xRoot.getRank() + 1)\n",
    "        \n",
    "def Find(x):\n",
    "    # Path compression - whenever you're finding the root of a node, set the parent to the root directly\n",
    "    if x.getParent() != x:\n",
    "        x.setParent(Find(x.getParent()))\n",
    "    return x.getParent()\n",
    "    \n",
    "def link_clusters(edge_list, node_names, k):\n",
    "    # edge_list is list of sorted edges in tuple form (weight, source, destination)\n",
    "    # node_names is the list of nodes' names\n",
    "    # k is the number of iterations\n",
    "\n",
    "    # get number of nodes\n",
    "    n = len(node_names)\n",
    "    \n",
    "    # initialise nodePointers dictionary of linked nodes\n",
    "    nodeList = []\n",
    "\n",
    "    # add all the node names as Nodes into the list and init MakeSet (each node is a set of itself at the beginning)\n",
    "    for i in range(n):\n",
    "        nodeList.append(Node(node_names[i]))\n",
    "        MakeSet(nodeList[i])        \n",
    "        \n",
    "    # loop this k times\n",
    "    for i in range(k):\n",
    "        # extract the k highest weights / correlations from the list\n",
    "        # Negative correlated nodes should be nearer to the end of the list (i.e. stocks that are dissimiliar to each other, in opposite direction)\n",
    "        # Negative weights should not affect the algorithm correctness\n",
    "        weight, source, dest = edge_list[i]\n",
    "        print(weight, source, dest) # for debugging purposes\n",
    "        Union(nodeList[source], nodeList[dest])\n",
    "        \n",
    "    return nodeList\n",
    "    \n",
    "    \n",
    "## UNCOMMENT THESE TO TEST ALGORITHM\n",
    "## We have compared the results of this algorithm with Part 3's algorithm. The results match.\n",
    "\n",
    "#nodeList = link_clusters(edge_list, all_stock_names, 99)\n",
    "\n",
    "## print result\n",
    "#for i in range(n):\n",
    "#    print(\"Node\", nodeList[i], \"is in same set as Node \", Find(nodeList[i]), \". Cluster of nodes are:\", Find(nodeList[i]).getSet())\n",
    "    \n",
    "\n",
    "##### END OF PART 4 part 1 #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're more interested in the \u001c",
    "financial applications of clustering, there are also opportunities to think about further steps. For example, some people claim that you can derive trading strategies based on clustering - that often one of the stocks in a cluster is a leader and the others follow that price. If this is true, you could track the price of the leader stock and then trade the other stocks in the cluster based on changes in the leader's price. Do you think this would make sense? Do you have an idea on how to identify a leader stock? \n",
    "\n",
    "You might also want to repeat the analysis for di\u001bfferent time periods. You would be able to do this by looking at the code for the second homework to \u001c",
    "gure out how to read data from Yahoo Finance using pandas, and going through the process for all companies in the csv \u001c",
    "le for another time period. Perhaps you could explore for example how correlations between companies have\n",
    "changed over time, or how clusters found by your algorithm change over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
